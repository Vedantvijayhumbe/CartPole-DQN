# Install required packages
!pip install gym==0.25.2 pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1


import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import collections
import random
from matplotlib import pyplot as plt
from IPython.display import clear_output

env = gym.make('CartPole-v1')
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

# H
learning_rate = 0.001
gamma = 0.99
batch_size = 64
memory_size = 10000
epsilon_start = 1.0
epsilon_end = 0.01
epsilon_decay = 0.995
target_update_frequency = 10
episodes = 200

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# nn for Q f(x) approximations 
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_dim)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Experience Replay Buffer
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)
    
    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        transitions = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*transitions)
        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), 
                np.array(next_states), np.array(dones, dtype=np.uint8))
    
    def __len__(self):
        return len(self.buffer)

# Initialize networks and buffer
policy_net = DQN(state_dim, action_dim).to(device)
target_net = DQN(state_dim, action_dim).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)
memory = ReplayBuffer(memory_size)
epsilon = epsilon_start

# Training function
def train():
    if len(memory) < batch_size:
        return
    
    # Sample batch from memory
    states, actions, rewards, next_states, dones = memory.sample(batch_size)
    
    # Convert to tensors
    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).to(device)
    rewards = torch.FloatTensor(rewards).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    dones = torch.FloatTensor(dones).to(device)
    
    # Current Q values
    current_q = policy_net(states).gather(1, actions.unsqueeze(1))
    
    # Next Q values from target network
    next_q = target_net(next_states).max(1)[0].detach()
    expected_q = rewards + (1 - dones) * gamma * next_q
    
    # Compute loss and update
    loss = nn.MSELoss()(current_q.squeeze(), expected_q)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Training loop
episode_rewards = []

for episode in range(episodes):
    state = env.reset()
    total_reward = 0
    
    while True:
        # Epsilon-greedy action selection
        if random.random() < epsilon:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                q_values = policy_net(state_tensor)
                action = q_values.argmax().item()
        
        # Take action
        next_state, reward, done, _ = env.step(action)
        total_reward += reward
        
        # Store transition
        memory.add(state, action, reward, next_state, done)
        
        # Train
        train()
        
        state = next_state
        
        if done:
            break
    
    # Update target network
    if episode % target_update_frequency == 0:
        target_net.load_state_dict(policy_net.state_dict())
    
    # Decay epsilon
    epsilon = max(epsilon_end, epsilon * epsilon_decay)
    
    episode_rewards.append(total_reward)
    
    # Print progress
    if (episode + 1) % 10 == 0:
        avg_reward = np.mean(episode_rewards[-10:])
        print(f"Episode {episode+1}, Average Reward (last 10): {avg_reward:.2f}, Epsilon: {epsilon:.2f}")
        
        # Plot rewards
        clear_output(wait=True)
        plt.figure(figsize=(10, 5))
        plt.plot(episode_rewards, label='Episode Reward')
        plt.plot([np.mean(episode_rewards[max(0, i-10):i+1]) for i in range(len(episode_rewards))], 
                label='Moving Average (10)')
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.legend()
        plt.title('Training Progress')
        plt.show()

def evaluate_agent(policy_net, num_episodes=100, render=False):
    """Evaluate the agent over multiple episodes"""
    total_rewards = []
    env = gym.make('CartPole-v1')
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        
        while True:
            if render and episode == 0: 
                env.render()
                
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                action = policy_net(state_tensor).argmax().item()
                
            state, reward, done, _ = env.step(action)
            episode_reward += reward
            
            if done:
                total_rewards.append(episode_reward)
                break
                
    env.close()
    
    avg_reward = np.mean(total_rewards)
    std_reward = np.std(total_rewards)
    max_reward = np.max(total_rewards)
    min_reward = np.min(total_rewards)
    
    print(f"Evaluation over {num_episodes} episodes:")
    print(f"Average reward: {avg_reward:.2f}")
    print(f"Standard deviation: {std_reward:.2f}")
    print(f"Maximum reward: {max_reward}")
    print(f"Minimum reward: {min_reward}")
    
    # Check if environment is solved
    if avg_reward >= 195:
        print(" Environment solved!")
    else:
        print("Environment not solved yet")
    
    return total_rewards

# Run test
test_agent()
# got much higher rewards using this one 
# Close environment
env.close()
